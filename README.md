# Data-Pipeline-with-Apache-Airflow
This repository contains code that builds a data pipeline with Apache Airflow to stage data from S3 bucket to AWS Redshift , Insert data into various tables in AWS redshift and perform data quality checks.

## Project Description

A startup wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. Their analytics team is particularly interested in understanding what songs users are listening to. The team wants to introduce more automation and monitoring to their data warehouse ETL pipelines and came to the conclusion that the best tool to achieve this is Apache Airflow.

As a data engineer I built a data pipelines using Apache Airflow that is dynamic and built from reusable tasks, can be monitored, allows easy backfills and performs data quality check. 

## Project Structure

* `data` - folder containing all the JSON logs that contain data about user activity on app and metadata about songs.
* `dags` - Folder containing python scripts where each Python script contains a unique DAG (Direct Acyclic Graph).
* `plugins` - Folder containing Custom Airflow operator classes that can be used to create Hooks to S3 and Redshift and also load the data into the tables in database.
* README.md - Provides a summary of the project and discussions on the data modelling.
* Resource - Folder containing images that were used in the README.

## Data Pipeline

The below image gives a visual of the data pipeline and the task dependencies as generated by Airflow based on the information that it received from the script containing the DAG.

* As soon as, Airflow Scheduler starts the DAG, the first task it does is that it creates all the required tables into the Postgres database that is hosted in the Redshift clusters.
* Next, Airflow Scheduler carries out the tasks of staging the data from both the log files present within the S3 bucket into the staging tables within Redshift in parallel.
* Airflow then inserts the data from the staging table into the Fact table of the database as per the principle of Star Schema design of relational databases.
* Once data is inserted into the fact table, Airflow inserts the data into the dimension tables which all relates back to the Fact table. (Primary key of each of the dimension table is registered as the foreign key in the Fact table).
* Finally, data quality checks are performed against each of the facts and dimension tables to ensure that they have actually been populated with data and that data inside is as expected.

## Running the Scripts

### Udacity Workspace

Just clone the contents of this repo and place it under your `/home/username/airflow/` path. Set up the AWS and Redshift connections with your details from the Airflow Webbrowser. You should see a DAG with the name `AirflowProject` show up, turn the DAG on and trigger it which will start the Data Pipeline.

### Locally in Ubuntu (Only works if you have S3 and Redshift Access)

Ensure you have ubuntu 16.04 or 18.04 installed and Python 3.6 or greater.

* Follow the instructions in this ![link](https://www.ryanmerlin.com/2019/07/apache-airflow-installation-on-ubuntu-18-04-18-10/) until but not including the secion **Scheduling Airflow to run as a background daemon with systemd**.
* Once set up as per previous instruction, clone the contents of this repo and place it in the path `/home/username/airflow`. 
* Execute the below commands in your Ubuntu terminal to start airflow in Daemon mode
```bash
airflow initdb
airflow scheduler -D
airflow webserver -D
```
* Open your browser and navigate to `localhost:8080` you should then see the Airflow UI load up. You should also see a DAG with the name `AirflowProject`. 
* Click on `Admin` in the UI and then click on `Connection` and then click on `Create`. Create the two connections shown below in the images with your AWS Details
* Turn the DAG ON and let Airflow start the data pipeline by clicking on the Run icon in the same row as the DAG.
