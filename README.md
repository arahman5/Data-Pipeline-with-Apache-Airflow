# Data-Pipeline-with-Apache-Airflow
This repository contains code that builds a data pipeline with Apache Airflow to stage data from S3 bucket to AWS Redshift , Insert data into various tables in AWS redshift and perform data quality checks.

## Project Description

A startup wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. Their analytics team is particularly interested in understanding what songs users are listening to. The team wants to introduce more automation and monitoring to their data warehouse ETL pipelines and came to the conclusion that the best tool to achieve this is Apache Airflow.

As a data engineer I built a data pipelines using Apache Airflow that is dynamic and built from reusable tasks, can be monitored, allows easy backfills and performs data quality check. 

## Project Structure

* `data` - folder containing all the JSON logs that contain data about user activity on app and metadata about songs.
* `dags` - Folder containing python scripts where each Python script contains a unique DAG (Direct Acyclic Graph).
* `plugins` - Folder containing Custom Airflow operator classes that can be used to create Hooks to S3 and Redshift and also load the data into the tables in database.
* README.md - Provides a summary of the project and discussions on the data modelling.
* Resource - Folder containing images that were used in the README.

## Data Pipeline

The below image gives a visual of the data pipeline and the task dependencies as generated by Airflow based on the information that it received from the script containing the DAG.

* As soon as, Airflow Scheduler starts the DAG, the first task it does is that it creates all the required tables into the Postgres database that is hosted in the Redshift clusters.
* Next, Airflow Scheduler carries out the tasks of staging the data from both the log files present within the S3 bucket into the staging tables within Redshift in parallel.
* Airflow then inserts the data from the staging table into the Fact table of the database as per the principle of Star Schema design of relational databases.
* Once data is inserted into the fact table, Airflow inserts the data into the dimension tables which all relates back to the Fact table. (Primary key of each of the dimension table is registered as the foreign key in the Fact table).
* Finally, data quality checks are performed against each of the facts and dimension tables to ensure that they have actually been populated with data and that data inside is as expected.
